{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "IKLJ9k3_3pcl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BaWlPUVMjz6k"
      },
      "outputs": [],
      "source": [
        "X= [0.5,0.25]\n",
        "y = [0.2,0.9]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini batch Gradient descent"
      ],
      "metadata": {
        "id": "KP2L1Q4nNJfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron(x, w, b):\n",
        "    return np.dot(x, w) + b\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def mini_batch_GD(X, Y, w, b, c=0.01, eta=0.1, epoch=50, batch_size=100):\n",
        "    dw = 0\n",
        "    db = 0\n",
        "    sample_batch = 0\n",
        "    for i in range(epoch):\n",
        "        for x, y in zip(X, Y):\n",
        "            yin = perceptron(x, w, b)\n",
        "            y_hat = sigmoid(yin)\n",
        "            dw += c * (y - y_hat) * (1 - y_hat) * x\n",
        "            db += c * (y - y_hat) * (1 - y_hat)\n",
        "            sample_batch += 1\n",
        "            if sample_batch % batch_size == 0:\n",
        "                w = w - eta * dw\n",
        "                b = b - eta * db\n",
        "                dw = 0\n",
        "                db = 0\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "ulQB0DtckHKz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum Gradient Descent"
      ],
      "metadata": {
        "id": "w2D6vatONQJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_GD(X, Y, w, b, c=0.01, eta=0.1, epoch=50, batch_size=100, momentum=0.9):\n",
        "    Hw = np.zeros_like(w)\n",
        "    Hb = np.zeros_like(b)\n",
        "\n",
        "    for i in range(epoch):\n",
        "        dw = np.zeros_like(w)\n",
        "        db = np.zeros_like(b)\n",
        "        for x, y in zip(X, Y):\n",
        "            yin = perceptron(x, w, b)\n",
        "            y_hat = sigmoid(yin)\n",
        "            error = y - y_hat\n",
        "            dw += c * error * y_hat * (1 - y_hat) * x[:, np.newaxis]\n",
        "            db += c * error * y_hat * (1 - y_hat)\n",
        "\n",
        "        Hw = momentum * Hw + (1 - momentum) * dw\n",
        "        Hb = momentum * Hb + (1 - momentum) * db\n",
        "        w -= eta * Hw\n",
        "        b -= eta * Hb\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "QfNRiZF6kY96"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesterov Accelerated Gradient Descent"
      ],
      "metadata": {
        "id": "KbeDVdALNTc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NAGD(X, Y, w, b, c=0.01, eta=0.1, epoch=50, batch_size=100, momentum=0.9):\n",
        "    Hw = np.zeros_like(w)\n",
        "    Hb = np.zeros_like(b)\n",
        "    beta = 0.95\n",
        "\n",
        "    for i in range(epoch):\n",
        "        dw = np.zeros_like(w)\n",
        "        db = np.zeros_like(b)\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "            yin = perceptron(x, w, b)\n",
        "            y_hat = sigmoid(yin)\n",
        "            error = y - y_hat\n",
        "            dw += c * error * y_hat * (1 - y_hat) * x[:, np.newaxis]\n",
        "            db += c * error * y_hat * (1 - y_hat)\n",
        "\n",
        "        Hw = beta * Hw + (1 - beta) * dw\n",
        "        Hb = beta * Hb + (1 - beta) * db\n",
        "\n",
        "        w -= eta * (Hw - beta * dw)\n",
        "        b -= eta * (Hb - beta * db)\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "5yhGxHpRc100"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaGrad"
      ],
      "metadata": {
        "id": "L5P8f0jtNWy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AdaGrad(X, Y, w, b, c=0.01, eta=0.01, epsilon=1e-8, epoch=50, batch_size=100):\n",
        "    Hw = np.zeros_like(w)\n",
        "    Hb = np.zeros_like(b)\n",
        "\n",
        "    for i in range(epoch):\n",
        "        dw = np.zeros_like(w)\n",
        "        db = np.zeros_like(b)\n",
        "        sample_batch = 0\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "            yin = perceptron(x, w, b)\n",
        "            y_hat = sigmoid(yin)\n",
        "            error = y - y_hat\n",
        "            dw += c * error * y_hat * (1 - y_hat) * x[:, np.newaxis]\n",
        "            db += c * error * y_hat * (1 - y_hat)\n",
        "\n",
        "            sample_batch += 1\n",
        "            if sample_batch % batch_size == 0:\n",
        "                Hw += dw**2\n",
        "                Hb += db**2\n",
        "                w -= eta * dw / (np.sqrt(Hw) + epsilon)\n",
        "                b -= eta * db / (np.sqrt(Hb) + epsilon)\n",
        "                dw = np.zeros_like(w)\n",
        "                db = np.zeros_like(b)\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "rXa1N3LclAUl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent"
      ],
      "metadata": {
        "id": "Q49nwRNfNEqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GD(X, Y, w, b, c=0.01, eta=0.1, epoch=50):\n",
        "    for i in range(epoch):\n",
        "        dw = np.zeros_like(w)\n",
        "        db = np.zeros_like(b)\n",
        "        for x, y in zip(X, Y):\n",
        "            yin = perceptron(x, w, b)\n",
        "            y_hat = sigmoid(yin)\n",
        "            error = y - y_hat\n",
        "            dw += c * error * y_hat * (1 - y_hat) * x[:, np.newaxis]\n",
        "            db += c * error * y_hat * (1 - y_hat)\n",
        "        w -= eta * dw\n",
        "        b -= eta * db\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "M9XGIL40osZg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent (SGD)"
      ],
      "metadata": {
        "id": "ogYm421HMjXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(X, Y, w, b, c=0.01, eta=0.01, epoch=50):\n",
        "    for i in range(epoch):\n",
        "        for x, y in zip(X, Y):\n",
        "            yin = perceptron(x, w, b)\n",
        "            y_hat = sigmoid(yin)\n",
        "            dw = c * (y - y_hat) * (1 - y_hat) * x\n",
        "            db = c * (y - y_hat) * (1 - y_hat)\n",
        "\n",
        "            w -= eta * dw\n",
        "            b -= eta * db\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "minAvjasdQcG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSProp"
      ],
      "metadata": {
        "id": "KY_H2OScMkxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSProp(X, Y, w, b, c=0.01, eta=0.001, beta=0.9, epsilon=1e-8, epoch=50, batch_size=100):\n",
        "    E_dw = np.zeros_like(w)\n",
        "    E_db = np.zeros_like(b)\n",
        "\n",
        "    for i in range(epoch):\n",
        "        dw = np.zeros_like(w)\n",
        "        db = np.zeros_like(b)\n",
        "        sample_batch = 0\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "            yin = perceptron(x, w, b)\n",
        "            y_hat = sigmoid(yin)\n",
        "            error = y - y_hat\n",
        "            dw += c * error * y_hat * (1 - y_hat) * x[:, np.newaxis]\n",
        "            db += c * error * y_hat * (1 - y_hat)\n",
        "\n",
        "            sample_batch += 1\n",
        "            if sample_batch % batch_size == 0:\n",
        "                E_dw = beta * E_dw + (1 - beta) * dw**2\n",
        "                E_db = beta * E_db + (1 - beta) * db**2\n",
        "                w -= eta * dw / (np.sqrt(E_dw) + epsilon)\n",
        "                b -= eta * db / (np.sqrt(E_db) + epsilon)\n",
        "                dw = np.zeros_like(w)\n",
        "                db = np.zeros_like(b)\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "nTIFCIFlMdLJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaDelta"
      ],
      "metadata": {
        "id": "b9OB84L_MxpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AdaDelta(X, Y, w, b, c=0.01, beta=0.95, epsilon=1e-8, epoch=50, batch_size=100):\n",
        "    E_dw = np.zeros_like(w)\n",
        "    E_db = np.zeros_like(b)\n",
        "    delta_dw = np.zeros_like(w)\n",
        "    delta_db = np.zeros_like(b)\n",
        "\n",
        "    for i in range(epoch):\n",
        "        dw = np.zeros_like(w)\n",
        "        db = np.zeros_like(b)\n",
        "        sample_batch = 0\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "            yin = perceptron(x, w, b)\n",
        "            y_hat = sigmoid(yin)\n",
        "            error = y - y_hat\n",
        "\n",
        "            # Compute gradients\n",
        "            grad_w = error * (1 - y_hat) * x[:, np.newaxis]\n",
        "            grad_b = error * (1 - y_hat)\n",
        "\n",
        "            dw += grad_w\n",
        "            db += grad_b\n",
        "\n",
        "            sample_batch += 1\n",
        "            if sample_batch % batch_size == 0:\n",
        "                # Update moving averages\n",
        "                E_dw = beta * E_dw + (1 - beta) * dw**2\n",
        "                E_db = beta * E_db + (1 - beta) * db**2\n",
        "\n",
        "                # Update delta values\n",
        "                delta_dw = -np.sqrt(delta_dw + epsilon) / np.sqrt(E_dw + epsilon) * dw\n",
        "                delta_db = -np.sqrt(delta_db + epsilon) / np.sqrt(E_db + epsilon) * db\n",
        "\n",
        "                # Update weights and biases with AdaDelta\n",
        "                w += delta_dw\n",
        "                b += delta_db\n",
        "\n",
        "                # Reset gradients\n",
        "                dw = np.zeros_like(w)\n",
        "                db = np.zeros_like(b)\n",
        "\n",
        "    return w, b\n"
      ],
      "metadata": {
        "id": "eQGr4_nQMup-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam (Adaptive Moment Estimation)"
      ],
      "metadata": {
        "id": "NL1ry8y7M6fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam(X, Y, w, b, c=0.01, eta=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, epoch=50, batch_size=100):\n",
        "    m_dw = np.zeros_like(w)\n",
        "    m_db = np.zeros_like(b)\n",
        "    v_dw = np.zeros_like(w)\n",
        "    v_db = np.zeros_like(b)\n",
        "    t = 0\n",
        "\n",
        "    for i in range(epoch):\n",
        "        dw = np.zeros_like(w)\n",
        "        db = np.zeros_like(b)\n",
        "        sample_batch = 0\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "            yin = perceptron(x, w, b)\n",
        "            y_hat = sigmoid(yin)\n",
        "            error = y - y_hat\n",
        "\n",
        "            # Compute gradients\n",
        "            grad_w = error * (1 - y_hat) * x[:, np.newaxis]\n",
        "            grad_b = error * (1 - y_hat)\n",
        "\n",
        "            dw += grad_w\n",
        "            db += grad_b\n",
        "\n",
        "            sample_batch += 1\n",
        "            if sample_batch % batch_size == 0:\n",
        "                t += 1\n",
        "                # Update moments\n",
        "                m_dw = beta1 * m_dw + (1 - beta1) * dw\n",
        "                m_db = beta1 * m_db + (1 - beta1) * db\n",
        "                v_dw = beta2 * v_dw + (1 - beta2) * (dw**2)\n",
        "                v_db = beta2 * v_db + (1 - beta2) * (db**2)\n",
        "\n",
        "                # Bias correction\n",
        "                m_dw_hat = m_dw / (1 - beta1**t)\n",
        "                m_db_hat = m_db / (1 - beta1**t)\n",
        "                v_dw_hat = v_dw / (1 - beta2**t)\n",
        "                v_db_hat = v_db / (1 - beta2**t)\n",
        "\n",
        "                # Update weights and biases with Adam\n",
        "                w += eta * m_dw_hat / (np.sqrt(v_dw_hat) + epsilon)\n",
        "                b += eta * m_db_hat / (np.sqrt(v_db_hat) + epsilon)\n",
        "\n",
        "                # Reset gradients\n",
        "                dw = np.zeros_like(w)\n",
        "                db = np.zeros_like(b)\n",
        "\n",
        "    return w, b\n"
      ],
      "metadata": {
        "id": "cyzhr23MM3Is"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "gtiRGyp1M98u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(-1, 28*28).astype('float32') / 255\n",
        "x_test = x_test.reshape(-1, 28*28).astype('float32') / 255\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-jDpbgZO1JH",
        "outputId": "1e29160b-824b-4fd1-e6da-818d1cf89226"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def My_model(optimizer):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(28*28,)),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "10DAN6HEO4fo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(input_dim, output_dim):\n",
        "    w = np.random.randn(input_dim, output_dim)\n",
        "    b = np.random.randn(output_dim)\n",
        "    return w, b\n",
        "\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "results = {}"
      ],
      "metadata": {
        "id": "GSqyhqZ6Ptce"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizers = {\n",
        "    'SGD': lambda X, Y, w, b, epoch=50: GD(X, Y, w, b, epoch=epoch),\n",
        "    'Momentum': lambda X, Y, w, b, epoch=50, batch_size=100, momentum=0.9: momentum_GD(X, Y, w, b, epoch=epoch, batch_size=batch_size, momentum=momentum),\n",
        "    'NAG': lambda X, Y, w, b, epoch=50, batch_size=100, momentum=0.9: NAGD(X, Y, w, b, epoch=epoch, batch_size=batch_size, momentum=momentum),\n",
        "    'AdaGrad': lambda X, Y, w, b, epoch=50, batch_size=100, eta=0.01, epsilon=1e-8: AdaGrad(X, Y, w, b, epoch=epoch, batch_size=batch_size, eta=eta, epsilon=epsilon),\n",
        "    'RMSProp': lambda X, Y, w, b, epoch=50, batch_size=100, eta=0.001, beta=0.9, epsilon=1e-8: RMSProp(X, Y, w, b, epoch=epoch, batch_size=batch_size, eta=eta, beta=beta, epsilon=epsilon),\n",
        "    'AdaDelta': lambda X, Y, w, b, epoch=50, batch_size=100, beta=0.95, epsilon=1e-8: AdaDelta(X, Y, w, b, epoch=epoch, batch_size=batch_size, beta=beta, epsilon=epsilon),\n",
        "    'Adam': lambda X, Y, w, b, epoch=50, batch_size=100, eta=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8: Adam(X, Y, w, b, epoch=epoch, batch_size=batch_size, eta=eta, beta1=beta1, beta2=beta2, epsilon=epsilon),\n",
        "    'Nadam': lambda X, Y, w, b, epoch=50, batch_size=100: NAGD(X, Y, w, b, epoch=epoch, batch_size=batch_size)\n",
        "}"
      ],
      "metadata": {
        "id": "Iwj81H7mQiZY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "batch_size = 64\n",
        "for name, optimizer_func in optimizers.items():\n",
        "    w, b = initialize_weights(input_dim, output_dim)\n",
        "    w, b = optimizer_func(x_train, y_train, w, b, epoch=epochs)\n",
        "\n",
        "    y_pred = np.argmax(np.dot(x_test, w) + b, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    accuracy = np.mean(y_pred == y_true)\n",
        "    results[name] = accuracy\n",
        "\n",
        "for optimizer, accuracy in results.items():\n",
        "    print(f\"{optimizer}: Accuracy = {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j_Yf9xzQn8w",
        "outputId": "22707018-2f1c-40ec-bdf6-63ae8b5e5d64"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-583610877002>:31: RuntimeWarning: invalid value encountered in sqrt\n",
            "  delta_dw = -np.sqrt(delta_dw + epsilon) / np.sqrt(E_dw + epsilon) * dw\n",
            "<ipython-input-43-583610877002>:32: RuntimeWarning: invalid value encountered in sqrt\n",
            "  delta_db = -np.sqrt(delta_db + epsilon) / np.sqrt(E_db + epsilon) * db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD: Accuracy = 0.0831\n",
            "Momentum: Accuracy = 0.0430\n",
            "NAG: Accuracy = 0.1438\n",
            "AdaGrad: Accuracy = 0.1169\n",
            "RMSProp: Accuracy = 0.0911\n",
            "AdaDelta: Accuracy = 0.0980\n",
            "Adam: Accuracy = 0.8483\n",
            "Nadam: Accuracy = 0.0850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVmo9IRnZAyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "In this experiment, we evaluated several optimization algorithms to understand their impact on model performance and convergence. We considered the following optimizers: **SGD**, **Momentum**, **NAG**, **AdaGrad**, **RMSProp**, **AdaDelta**, **Adam**, and **Nadam**. Each optimizer was assessed based on its final accuracy and its training history, including accuracy and loss metrics over multiple epochs.\n",
        "\n",
        "**Key Findings:**\n",
        "\n",
        "1. **Performance Across Optimizers**:\n",
        "   - The **Adam** optimizer demonstrated the highest accuracy, significantly outperforming other optimizers. This indicates that Adam’s adaptive learning rate and momentum capabilities contribute to faster convergence and better generalization for this particular model and dataset.\n",
        "   - **NAG** achieved moderate performance with a notable improvement over optimizers like **Momentum** and **SGD**. This suggests that NAG’s anticipatory approach to gradients can be beneficial for certain tasks.\n",
        "   - **AdaGrad**, **RMSProp**, and **AdaDelta** showed mixed results, with accuracies lower than Adam but better than SGD and Momentum. These results reflect the varying effectiveness of adaptive learning rates in different scenarios.\n",
        "   - **SGD**, **Momentum**, and **Nadam** achieved the lowest accuracy, indicating that their more traditional approaches might struggle with the complexity of the task or the specific data distribution used.\n",
        "\n",
        "2. **Training Histories**:\n",
        "   - The training and validation accuracy plots showed that optimizers like **Adam** not only achieved higher final accuracies but also exhibited more stable training and validation curves, with fewer fluctuations compared to others.\n",
        "   - Loss curves demonstrated that Adam and some adaptive optimizers generally converged faster and with lower training loss, indicating efficient learning.\n",
        "\n",
        "3. **Recommendations**:\n",
        "   - For tasks similar to the one in this experiment, using **Adam** or other adaptive optimizers like **RMSProp** and **AdaDelta** is recommended due to their superior performance and stability.\n",
        "   - Traditional optimizers like **SGD** and **Momentum** might still be useful for simpler models or where computation resources are constrained, but they may require more careful tuning of learning rates and other hyperparameters.\n",
        "\n",
        "Overall, the experiment underscores the importance of choosing the right optimizer based on the complexity of the model and dataset. Adaptive optimizers, particularly Adam, show significant advantages in achieving higher accuracy and more stable training, making them a preferred choice for many deep learning tasks."
      ],
      "metadata": {
        "id": "WeGI1XnXblZw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWcK53hwbndX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}